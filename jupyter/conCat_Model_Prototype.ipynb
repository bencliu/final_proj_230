{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chemical-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, BatchNormalization, Flatten, Dense, TimeDistributed, Lambda, \\\n",
    "    MaxPooling2D, BatchNormalization, concatenate, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "novel-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConcatDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, list_IDs, labels, batch_size, dim, n_channels, n_metaFeatures, n_classes, shuffle=True):\n",
    "        #Initialization\n",
    "        self.dim = dim # (H, W)\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_metaFeatures = n_metaFeatures\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denote: Number of batches per epoch\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        #Updates indexes: Called at the VERY beginning | + end of each epoch\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    #Private helper method\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        #Generates data containing batch_size samples | X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        image_init = np.empty((self.batch_size, *self.dim, self.n_channels)) # (numSamples, H, W, C)\n",
    "        meta_init = np.empty((self.batch_size, self.n_metaFeatures))\n",
    "        y = np.empty((self.batch_size), dtype=int) # (numSamples) => Labels\n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "\n",
    "            # extract raw image and metadata\n",
    "            input_image = raw_input_images[ID] # from global variable definition\n",
    "            input_metadata = raw_input_metadata[ID] # from global variable definition\n",
    "            image_init[i, ] = input_image\n",
    "            meta_init[i, ] = input_metadata \n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID] - 1 #Store label\n",
    "\n",
    "        # assemble X tuple\n",
    "        X = (image_init, meta_init)\n",
    "        \n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "nonprofit-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "\n",
    "class ConcatProtypeModel():\n",
    "    def __init__(self):\n",
    "        self.numExamples = 1000\n",
    "        self.width = 50\n",
    "        self.height = 50\n",
    "        self.numChannels = 7\n",
    "        self.numMetaFeatures = 5\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.validation_generator = None\n",
    "        self.train_generator = None\n",
    "        self.genParams = {'dim': (self.width, self.height),\n",
    "                          'batch_size': 16,\n",
    "                          'n_classes': 10,\n",
    "                          'n_channels': self.numChannels,\n",
    "                          'n_metaFeatures': self.numMetaFeatures,\n",
    "                          'shuffle': True}\n",
    "        \n",
    "    def generate_fake_dataset(self):\n",
    "        raw_input_images = np.random.randint(low = 0, high = 5, size = (self.numExamples, self.height, self.width, self.numChannels))\n",
    "        raw_input_metadata = np.random.randint(low = 0, high = 5, size = (self.numExamples, self.numMetaFeatures))\n",
    "        labels = np.random.randint(low = 0, high = 5, size = (self.numExamples, 1))\n",
    "        return raw_input_images, raw_input_metadata, labels\n",
    "    \n",
    "    def generate_fake_partition_dict(self):\n",
    "        idArray = list(range(numExamples))\n",
    "        ten_percent_split = (-1) * int(len(idArray) / 10)\n",
    "        twent_percent_split = (-2) * int(len(idArray) / 10)\n",
    "        test_ids = idArray[ten_percent_split:]\n",
    "        val_ids = idArray[twent_percent_split:ten_percent_split]\n",
    "        train_ids = idArray[:twent_percent_split]\n",
    "        partition = {'train': train_ids, 'val': val_ids, 'test': test_ids}\n",
    "        return partition\n",
    "        \n",
    "    def compile(self):\n",
    "        # inputs\n",
    "        input_images = Input(shape = (self.height, self.width, self.numChannels), name = \"input_images\")\n",
    "        input_metadata = Input(shape = (self.numMetaFeatures), name = \"input_metadata\")\n",
    "        print(\"input_image shape per batch:\" + str(input_images.get_shape()))\n",
    "        print(\"input_metadata shape per batch:\" + str(input_metadata.get_shape()))\n",
    "        \n",
    "        # image arm\n",
    "        # Note: this is identical to the vanilla CNN model architecture\n",
    "        im_conv2D_1_1 = Conv2D(64, 3, activation=\"relu\", padding=\"same\", input_shape=(self.width, self.height, 7), name = \"image_conv2D_1_1\")(input_images)\n",
    "        im_conv2D_1_2 = Conv2D(64, 3, activation=\"relu\", padding=\"same\", name = \"image_conv2D_1_2\")(im_conv2D_1_1)\n",
    "        im_pool_1 = MaxPooling2D(2, name = \"image_pool_1\")(im_conv2D_1_2)\n",
    "        im_conv2D_2_1 = Conv2D(128, 3, activation=\"relu\", padding=\"same\", name = \"image_conv2D_2_1\")(im_pool_1)\n",
    "        im_conv2D_2_2 = Conv2D(128, 3, activation=\"relu\", padding=\"same\", name = \"image_conv2D_2_2\")(im_conv2D_2_1)\n",
    "        im_pool_2 = MaxPooling2D(2, name = \"image_pool_2\")(im_conv2D_2_2)\n",
    "\n",
    "        # metadata arm\n",
    "        meta_dense_1 = Dense(128, activation = \"relu\", name = \"meta_dense_1\")(input_metadata)\n",
    "        meta_drop_1 = Dropout(0.5, name = \"meta_drop_1\")(meta_dense_1)\n",
    "        meta_batch_1 = BatchNormalization(name = \"meta_batch_1\")(meta_drop_1)\n",
    "        meta_dense_2 = Dense(64, activation = \"relu\", name = \"meta_dense_2\")(meta_batch_1)\n",
    "        meta_drop_2 = Dropout(0.5, name = \"meta_drop_2\")(meta_dense_2)\n",
    "        meta_batch_2 = BatchNormalization(name = \"meta_batch_2\")(meta_drop_2)\n",
    "\n",
    "        # concatenate\n",
    "        flatten_images = Flatten(name = \"flatten_images\")(im_pool_2)\n",
    "        flatten_metadata = Flatten(name = \"flatten_metadata\")(meta_batch_2)\n",
    "        concat = concatenate([flatten_images, flatten_metadata], name = \"concat\") \n",
    "        cat_dense_1 = Dense(128, activation = \"relu\", name = \"cat_dense_1\")(concat)\n",
    "        cat_drop_1 = Dropout(0.5, name = \"cat_drop_1\")(cat_dense_1)\n",
    "        cat_batch_1 = BatchNormalization(name = \"cat_batch_1\")(cat_drop_1)\n",
    "        cat_dense_2 = Dense(64, activation = \"relu\", name = \"cat_dense_2\")(cat_batch_1)\n",
    "        cat_drop_2 = Dropout(0.5, name = \"cat_drop_2\")(cat_dense_2)\n",
    "        cat_batch_2 = BatchNormalization(name = \"cat_batch_2\")(cat_drop_2)\n",
    "        \n",
    "        # output\n",
    "        output = Dense(10, activation = \"softmax\", name = \"output\")(cat_batch_2)\n",
    "        self.model = keras.Model(inputs = [input_images, input_metadata], outputs = [output], name = \"concat_model\")\n",
    "    \n",
    "        # output model summary\n",
    "        self.model.summary()\n",
    "        \n",
    "        # compile model\n",
    "        self.model.compile(loss=\"categorical_crossentropy\",\n",
    "                      optimizer=optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999),\n",
    "                      metrics=[\"accuracy\"])\n",
    "    \n",
    "    def train(self, partition, labels):\n",
    "\n",
    "        # Generators\n",
    "        self.train_generator = ConcatDataGenerator(partition['train'], labels, **self.genParams)\n",
    "        self.validation_generator = ConcatDataGenerator(partition['val'], labels, **self.genParams)\n",
    "        \n",
    "        # train model\n",
    "        self.history = self.model.fit(x = self.train_generator,\n",
    "                                      epochs = 2,\n",
    "                                      verbose = 1,\n",
    "                                      validation_data = self.validation_generator,\n",
    "                                      shuffle = True)\n",
    "        \n",
    "    def eval(self):\n",
    "        None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "conditional-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data generation\n",
    "num_examples = 1000\n",
    "height = 50\n",
    "width = 50\n",
    "bands = 7\n",
    "metaFeatures = 5\n",
    "raw_input_images = np.random.randint(low = 0, high = 5, size = (num_examples, height, width, bands))\n",
    "raw_input_metadata = np.random.randint(low = 0, high = 5, size = (num_examples, metaFeatures))\n",
    "labels = np.random.randint(low = 0, high = 5, size = (num_examples, 1))\n",
    "\n",
    "# generate fake partition dictionary\n",
    "idArray = list(range(num_examples))\n",
    "ten_percent_split = (-1) * int(len(idArray) / 10)\n",
    "twent_percent_split = (-2) * int(len(idArray) / 10)\n",
    "test_ids = idArray[ten_percent_split:]\n",
    "val_ids = idArray[twent_percent_split:ten_percent_split]\n",
    "train_ids = idArray[:twent_percent_split]\n",
    "partition_dict = {'train': train_ids, 'val': val_ids, 'test': test_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "diverse-rogers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_image shape per batch:(None, 50, 50, 7)\n",
      "input_metadata shape per batch:(None, 5)\n",
      "Model: \"concat_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_images (InputLayer)       [(None, 50, 50, 7)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_metadata (InputLayer)     [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_conv2D_1_1 (Conv2D)       (None, 50, 50, 64)   4096        input_images[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "meta_dense_1 (Dense)            (None, 128)          768         input_metadata[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "image_conv2D_1_2 (Conv2D)       (None, 50, 50, 64)   36928       image_conv2D_1_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "meta_drop_1 (Dropout)           (None, 128)          0           meta_dense_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "image_pool_1 (MaxPooling2D)     (None, 25, 25, 64)   0           image_conv2D_1_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "meta_batch_1 (BatchNormalizatio (None, 128)          512         meta_drop_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "image_conv2D_2_1 (Conv2D)       (None, 25, 25, 128)  73856       image_pool_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "meta_dense_2 (Dense)            (None, 64)           8256        meta_batch_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "image_conv2D_2_2 (Conv2D)       (None, 25, 25, 128)  147584      image_conv2D_2_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "meta_drop_2 (Dropout)           (None, 64)           0           meta_dense_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "image_pool_2 (MaxPooling2D)     (None, 12, 12, 128)  0           image_conv2D_2_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "meta_batch_2 (BatchNormalizatio (None, 64)           256         meta_drop_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_images (Flatten)        (None, 18432)        0           image_pool_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_metadata (Flatten)      (None, 64)           0           meta_batch_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 18496)        0           flatten_images[0][0]             \n",
      "                                                                 flatten_metadata[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "cat_dense_1 (Dense)             (None, 128)          2367616     concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "cat_drop_1 (Dropout)            (None, 128)          0           cat_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cat_batch_1 (BatchNormalization (None, 128)          512         cat_drop_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cat_dense_2 (Dense)             (None, 64)           8256        cat_batch_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cat_drop_2 (Dropout)            (None, 64)           0           cat_dense_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cat_batch_2 (BatchNormalization (None, 64)           256         cat_drop_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 10)           650         cat_batch_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,649,546\n",
      "Trainable params: 2,648,778\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      "50/50 [==============================] - 20s 370ms/step - loss: 2.9670 - accuracy: 0.1361 - val_loss: 2.6807 - val_accuracy: 0.1667\n",
      "Epoch 2/2\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 2.7027 - accuracy: 0.1034 - val_loss: 2.1353 - val_accuracy: 0.1562\n"
     ]
    }
   ],
   "source": [
    "# Test Model \n",
    "NN = ConcatProtypeModel()\n",
    "\n",
    "# Generate fake dataset for testing\n",
    "# raw_input_images, raw_input_metadata, labels = NN.generate_fake_dataset()\n",
    "\n",
    "# Generate fake partition\n",
    "# partition_dict = NN.generate_fake_partition_dict()\n",
    "\n",
    "# Define and compile NN\n",
    "NN.compile()\n",
    "\n",
    "# Train Model\n",
    "# NN.train(partition_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-scott",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
